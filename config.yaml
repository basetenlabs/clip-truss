data_dir: data
input_type: Any
model_class_filename: model.py
model_class_name: Model
model_framework: custom
model_type: custom
description: > 
  This model provides zero-shot-like functionality for classifying images. This is an
  implementation of the CLIP (Contrastive Language-Image Pre-Training)
  neural network that has been trained on image and text pairs by OpenAI.
model_metadata:
  pretty_name: "CLIP: Connecting text and images"
  tags:
  - image-classification
model_module_dir: model
model_name: clip
python_version: py39
requirements:
  - torchvision==0.8.2
  - git+https://github.com/openai/CLIP.git
resources:
  cpu: 3000m
  memory: 8Gi
  use_gpu: true
system_packages: []
spec_version: 2.0
